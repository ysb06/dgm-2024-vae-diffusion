vae:
  model:  # VAE specific params. Check the `main/models/vae.py`
    input_res: 32   # Input resolution 데이터셋의 이미지 해상도(Image Size)와 같음
    enc_block_str : "32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3"
    enc_channel_str: "32:64,16:128,8:256,4:256,1:512"
    dec_block_str: "1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15"
    dec_channel_str: "32:64,16:128,8:256,4:256,1:512"
    lr: 1e-4
    alpha: 1.0  # Beta-VAE, 베타 값으로 reconstruction loss에 가중치를 주어 조절. Baseline에서는 1.0으로 일반적인 VAE를 사용
    # DDPM과 함께 학습했을 때 성능이 떨어지는 부분은 epoch수 차이에 따른 문제일 수 있음
    # lr 조정과 step_lr scheduler를 별개로 사용하여 따로 학습하는 것과 유사한 속도로 학습하게 하면 좋을 것 같음


ddpm:
  decoder:
    in_channels: 3                # config.ddpm.data.n_channels
    model_channels: 128           # config.ddpm.model.dim
    out_channels: 3               # Fixed to 3
    num_res_blocks: 2             # config.ddpm.model.n_residual
    attention_resolutions: "16,"  # config.ddpm.model.attn_resolutions (need __parse_str)
    channel_mult: "1,2,2,2"       # config.ddpm.model.dim_mults (need __parse_str)
    use_checkpoint: false         # Fixed to False
    dropout: 0.3                  # config.ddpm.model.dropout
    num_heads: 8                  # config.ddpm.model.n_heads
    z_dim: 512                    # config.ddpm.training.z_dim
    use_scale_shift_norm: false   # config.ddpm.training.z_cond
    use_z: false                  # config.ddpm.training.z_cond

  model:   # UNet specific params. Check the DDPM implementation for details on these
    beta_1: 0.0001
    beta_2: 0.02
    T: 1000
  
  wrapper:
    lr: 2e-4                    # config.ddpm.training.lr
    cfd_rate: 0.0               # config.ddpm.training.cfd_rate, Conditioning signal dropout rate as in Classifier-free guidance
    n_anneal_steps: 5000        # config.ddpm.training.n_anneal_steps, number of warmup steps
    loss: "l2"                  # config.ddpm.training.loss, Diffusion loss type. Among ['l2', 'l1']
    conditional: true           # config.ddpm.training.type != 'uncond'
    grad_clip_val: 1.0          # config.ddpm.training.grad_clip
    z_cond: false               # config.ddpm.training.z_cond
  
  test_wrapper:
    checkpoint_path: ./outputs/checkpoints/diffuse_vae-epoch=474-loss=0.0000.ckpt
    conditional: true
    pred_steps: 1000
    eval_mode: sample
    resample_strategy: spaced
    skip_strategy: uniform
    sample_method: ddpm
    sample_from: target
    data_norm: false
    temp: 1.0
    guidance_weight: 0.0
    z_cond: true
    strict: true

image_writer:
  output_dir: ./outputs/hybrid_vd
  write_interval: batch
  n_steps: 1000
  eval_mode: sample
  conditional: true
  sample_prefix: hybrid
  save_vae: false
  save_mode: image
  is_norm: true

trainer:
  default_root_dir: ./outputs/hybrid_vd/sample2   # config.training.results_dir
  log_every_n_steps: 1          # config.training.log_step
  devices: 1
  accelerator: "gpu"            # TPU를 사용할 경우 devices와 함께 수정이 필요
  precision: 32                 # config.training.fp16 == True
vae:
  model:  # VAE specific params. Check the `main/models/vae.py`
    input_res: 32   # Input resolution 데이터셋의 이미지 해상도(Image Size)와 같음
    enc_block_str : "32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3"
    enc_channel_str: "32:64,16:128,8:256,4:256,1:512"
    dec_block_str: "1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15"
    dec_channel_str: "32:64,16:128,8:256,4:256,1:512"
    lr: 1e-4
    alpha: 1.0  # Beta-VAE, 베타 값으로 reconstruction loss에 가중치를 주어 조절. Baseline에서는 1.0으로 일반적인 VAE를 사용
    # DDPM과 함께 학습했을 때 성능이 떨어지는 부분은 epoch수 차이에 따른 문제일 수 있음
    # lr 조정과 step_lr scheduler를 별개로 사용하여 따로 학습하는 것과 유사한 속도로 학습하게 하면 좋을 것 같음


ddpm:
  decoder:
    in_channels: 3                # config.ddpm.data.n_channels
    model_channels: 128           # config.ddpm.model.dim
    out_channels: 3               # Fixed to 3
    num_res_blocks: 2             # config.ddpm.model.n_residual
    attention_resolutions: "16,"  # config.ddpm.model.attn_resolutions (need __parse_str)
    channel_mult: "1,2,2,2"       # config.ddpm.model.dim_mults (need __parse_str)
    use_checkpoint: false         # Fixed to False
    dropout: 0.3                  # config.ddpm.model.dropout
    num_heads: 8                  # config.ddpm.model.n_heads
    z_dim: 512                    # config.ddpm.training.z_dim
    use_scale_shift_norm: false   # config.ddpm.training.z_cond
    use_z: false                  # config.ddpm.training.z_cond

  model:   # UNet specific params. Check the DDPM implementation for details on these
    beta_1: 0.0001
    beta_2: 0.02
    T: 1000
  
  wrapper:
    lr: 2e-4                    # config.ddpm.training.lr
    cfd_rate: 0.0               # config.ddpm.training.cfd_rate, Conditioning signal dropout rate as in Classifier-free guidance
    n_anneal_steps: 5000        # config.ddpm.training.n_anneal_steps, number of warmup steps
    loss: "l2"                  # config.ddpm.training.loss, Diffusion loss type. Among ['l2', 'l1']
    conditional: true           # config.ddpm.training.type != 'uncond'
    grad_clip_val: 1.0          # config.ddpm.training.grad_clip
    z_cond: false               # config.ddpm.training.z_cond


dataset:
  name: "cifar10"             # config.vae.data.name
  root: "./datasets"            # config.vae.data.root
  image_size: 32                # config.vae.data.image_size
  norm: false                   # config.ddpm.data.norm, vae에서는 False로 설정. ddpm 레이어에서 정규화를 수행해야 하는지 확인 필요
  flip: false                   # config.vae.data.hflip, ddpm에서는 Augmentation을 위해 true로 설정. 이것을 반영할 방법 필요


dataloader:
  batch_size: 32                # config.ddpm.training.batch_size
  num_workers: 2                # config.ddpm.training.workers
  pin_memory: true              # Fixed to True
  shuffle: true                 # Fixed to True
  drop_last: true               # Fixed to True
  persistent_workers: true     # Fixed to True to use GPU


trainer:
  default_root_dir: ./outputs   # config.training.results_dir
  max_epochs: 1000              # VAE에서 2000근처에서 overfit이 발생함. DDPM은 5000까지 학습이 가능하므로 이 문제를 해결할 방법을 찾아야 함
  log_every_n_steps: 1          # config.training.log_step
  devices: 1
  accelerator: "gpu"            # TPU를 사용할 경우 devices와 함께 수정이 필요
  # tpu_cores: 8                # Pytorch Lightning에서 TPU 사용 시 이 변수를 사용할지 devices에 통합되었는지 확인 필요
  precision: 16                 # config.training.fp16 == True

use_ema: true                   # config.ddpm.training.use_ema
ema_decay: 0.9999               # config.ddpm.training.ema_decay
results_dir: ./outputs          # config.ddpm.training.results_dir
chkpt_interval: 1               # config.ddpm.training.chkpt_interval
ckpt_path: null
seed: 42